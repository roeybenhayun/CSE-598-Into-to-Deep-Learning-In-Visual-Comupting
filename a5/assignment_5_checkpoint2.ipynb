{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_fjiqi8k_kl"
      },
      "source": [
        "## Generative Adversarial Network\n",
        "\n",
        "Generative Adversarial Networks a.k.a GANs, are popular generative neural networks. GANs have demonstrated their effectiveness in nearly every problem in computer vision. The GAN works by training a pair of networks, Generator and Discriminator, with competing loss terms. As an analogy, we can think of these networks as an art-forger and the other, an art-expert. In GAN literature the Generator is the art-forger and the Discriminator is the art-expert. The Generator is trained to produce fake images (forgeries) to deceive the art-expert (Discriminator). The Discriminator which receives both the real images and fake images tries to distinguish between them to identify the fake images. The Generator uses the feedback from the Discriminator to improve it generation. Both the models are trained simulataneously and are always in competition with each other. This competition between the Generator and Discriminator drives them to improve their respective models continuously. The  model converges when the Generator produces fake images that are indistinguishable from the real images.  <br>\n",
        "\n",
        "In this setup, the Generator does not have access to the real images whereas the Discriminator has access to both the real and the generated fake images. \n",
        "\n",
        "Let us define Discriminator D that takes an image as input and produces a number **(0/1)** as output and a Generator G that takes random noise as input and outputs a fake image. In practice, G and D are trained alternately i.e., For a fixed generator G, the Discriminator D is trained to classify the training data as real (output a value close to 1) or fake(output a value close to 0). Subsequenty, we freeze the Discriminator and train the Generator G to produce an image (fake) that outputs a value close to 1 (real) when passed through the Discriminator D. Thus, if the Generator is perfectly trained then the Discriminator D will be maximally confused by the images generated by G and predict 0.5 for all the inputs. \n",
        "\n",
        "**It will be ideal to solve this assignemnet on a computer with a GPU**. The Coursera platform does not support a GPU. You may want to explore [Google Colab](https://www.youtube.com/watch?v=inN8seMm7UI&ab_channel=TensorFlow) or [Kaggle](https://www.kaggle.com/dansbecker/running-kaggle-kernels-with-a-gpu)\n",
        "\n",
        "Along with submitting the Python notebook, save the notebook along with its output after executing all the cells as a .html file and submit the html file as well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "cXGRROAek_kq",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ddbdf5ba0012a112b6ee61f91967fea3",
          "grade": false,
          "grade_id": "cell-ffaf9239ea94ed01",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "In this assignment, we will implement a Generative Adversarial Network on MNIST data and generate images that resemble the digits from the MNIST dataset.\n",
        "\n",
        "To implement a GAN, we basically require 5 components:\n",
        "\n",
        "- Real Dataset (real distribution)\n",
        "- Low dimensional random noise that is input to the Generator to produce fake images\n",
        "- Generator that generates fake images\n",
        "- Discriminator that acts as an expert to distinguish real and fake images.\n",
        "- Training loop where the competition occurs and models better themselves.\n",
        "\n",
        "\n",
        "Let us implement each of the parts and train the overall model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eaq6jV13k_kq",
        "outputId": "ac68c990-6e99-4d03-c379-8447133deda5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No gpu! only cpu ;)\n"
          ]
        }
      ],
      "source": [
        "## import packages\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "import torchvision.datasets as dset\n",
        "import os\n",
        "import numpy.testing as npt\n",
        "#from torchsummary import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "## Checks for the availability of GPU \n",
        "is_cuda = torch.cuda.is_available()\n",
        "is_cuda = False\n",
        "if is_cuda:\n",
        "    print(\"working on gpu!\")\n",
        "else:\n",
        "    print(\"No gpu! only cpu ;)\")\n",
        "    \n",
        "## The following random seeds are just for deterministic behaviour of the code and evaluation\n",
        "\n",
        "##############################################################################\n",
        "################### DO NOT MODIFY THE CODE BELOW #############################    \n",
        "##############################################################################\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "############################################################################### "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "T7rD7l-ok_kr",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "fb5762fcd78a47cde4fcc829462aba4c",
          "grade": false,
          "grade_id": "cell-8fdfedbfad91f5aa",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Download Data and Setup DataLoader\n",
        "\n",
        "In this step we work on preparing the data. We normalize the images to range [-1, +1] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDjG2O_gk_ks",
        "outputId": "ed8fe56d-eaf0-433a-ae3e-7a2093e331ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 427562102.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 35265121.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 210985429.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 4542/4542 [00:00<00:00, 20202045.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "\n",
        "root = './data/'\n",
        "if not os.path.isdir(root):\n",
        "    os.mkdir(root)\n",
        "\n",
        "train_bs = 128\n",
        "\n",
        "# Data transformation for the DataLoader - normalizes to between [-1,1]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.5], std=[0.5])])\n",
        "\n",
        "training_data = torchvision.datasets.MNIST(root, train=True, transform=transform,download=True)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=training_data, batch_size=train_bs, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "A5MKCoUCk_ks",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "79f1f2457b25d2bfe34a2d95f2f3bdd5",
          "grade": false,
          "grade_id": "cell-5e4f4ae980e68fd9",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Noise Input for the Generator \n",
        "\n",
        "Let us define a function which takes (batchsize, dimension) as input and returns a random noise of requested dimensions. This noise tensor will be the input to the generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "T7tYMd22k_ks",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cce412dd60cd120be65b5ddd632f5a86",
          "grade": false,
          "grade_id": "cell-a6e829041fba3bdc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def noise(bs, dim):\n",
        "    \"\"\"Generate random Gaussian noise vectors N(0,I), with mean 0 and variance 1.\n",
        "    \n",
        "    Inputs:\n",
        "    - bs: integer giving the batch size of noise to generate.\n",
        "    - dim: integer giving the dimension of the Gaussain noise to generate.\n",
        "    \n",
        "    Returns:\n",
        "    A PyTorch Tensor containing Gaussian noise with shape [bs, dim]\n",
        "    \"\"\"\n",
        "    \n",
        "    out = (torch.randn((bs, dim)))\n",
        "    if is_cuda:\n",
        "        out = out.cuda()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "D4jf1P7uk_kt",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "8659eca2e2ea26a764851d43fcba2817",
          "grade": false,
          "grade_id": "cell-db9437aa1b2c10d5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Generator Architecture - 20 points\n",
        "\n",
        "Define a Generator with the following architecture. \n",
        "\n",
        "- Linear layer (noise_dim -> 256)\n",
        "- LeakyReLU (works well for the Generators, we will use negative_slope=2)\n",
        "- Linear Layer (256 -> 512)\n",
        "- LeakyReLU\n",
        "- Linear Layer (512 -> 1024)\n",
        "- LeakyReLU\n",
        "- Linear Layer (1024 -> 784) (784 is the MNIST image size 28*28)\n",
        "- TanH (To scale the generated images to [-1,1], the same as real images)\n",
        "\n",
        "- LeakyRELU: https://pytorch.org/docs/stable/nn.html#leakyrelu \n",
        "- Fully connected layer: https://pytorch.org/docs/stable/nn.html#linear \n",
        "- TanH activation: https://pytorch.org/docs/stable/nn.html#tanh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "deletable": false,
        "id": "azNPW3Jfk_kt",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4e14069fcf14b1830302d1e0d53d2436",
          "grade": false,
          "grade_id": "Generator_define_soln",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim=100, out_size=784):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        '''\n",
        "        REST OF THE MODEL HERE\n",
        "        \n",
        "        # define a fully connected layer (self.layer1) from noise_dim -> 256 neurons      \n",
        "        # define a leaky relu layer(self.leaky_relu) with negative slope=0.2. We can reuse the same layer multiple times.\n",
        "        # define a fully connected layer (self.layer2) from 256 -> 512 neurons\n",
        "        # define a fully connected layer (self.layer3) from 512 -> 1024 neurons\n",
        "        # define a fully connected layer (self.layer4) from 1024 -> out_size neurons\n",
        "        # define a tanh activation function (self.tanh)\n",
        "        \n",
        "        '''\n",
        "        # your code here\n",
        "        self.layer1 = nn.Linear(noise_dim,256)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "        self.layer2 = nn.Linear(256,512)\n",
        "        self.layer3 = nn.Linear(512,1024)\n",
        "        self.layer4 = nn.Linear(1024,out_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Make a forward pass of the input through the generator. Leaky relu is used as the activation \n",
        "        function in all the intermediate layers. Tanh activation function is only used at the end (which\n",
        "        is after self.layer4)\n",
        "        \n",
        "        Note that, generator takes an random noise as input and gives out fake \"images\". Hence, the Tensor \n",
        "        output after tanh activation function should be reshaped into the same size as the real images. i.e., \n",
        "        [batch_size, n_channels, H, W] == (batch_size, 1,28,28). You may use the .view(.) function to acheive it.\n",
        "         \n",
        "        '''\n",
        "        # your code here\n",
        "        #print(\"X Input shape and size - before\")\n",
        "        #print(x.shape)\n",
        "        #print(x.size())\n",
        "        x = self.layer1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.tanh(x)\n",
        "        \n",
        "        #print(\"X Input shape and size - after\")\n",
        "        #print(x.shape)\n",
        "        #print(x.size())\n",
        "        W,H = x.size()\n",
        "\n",
        "        x = x.view(W,1,28,28)\n",
        "        #print(x.shape)\n",
        "        return x\n",
        "             "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "v22egDHgk_kt",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "32f6b2e3939a0e97d75702c50fb364ed",
          "grade": false,
          "grade_id": "cell-39709c8021fe3241",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "36c9008f-bb88-4a7c-f1cb-2600ea0480e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator(\n",
            "  (layer1): Linear(in_features=100, out_features=256, bias=True)\n",
            "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
            "  (layer2): Linear(in_features=256, out_features=512, bias=True)\n",
            "  (layer3): Linear(in_features=512, out_features=1024, bias=True)\n",
            "  (layer4): Linear(in_features=1024, out_features=784, bias=True)\n",
            "  (tanh): Tanh()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Generator and move it to GPU (if is_cuda)\n",
        "generator = Generator()\n",
        "print(generator)\n",
        "# If you have a system with a GPU, you may want to install torchsummary and display the network in more detail \n",
        "# summary(generator,(100,), device='cpu')\n",
        "\n",
        "# move to GPU\n",
        "if is_cuda:\n",
        "    generator = generator.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Sqo14nUCk_ku",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "bae22247cc766fb2327fa432c949124c",
          "grade": true,
          "grade_id": "Generator_define",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Test cases\n",
        "# Note the testcases only tests for input and output dimensions and range of values. \n",
        "# You may modify the architecture within those constraints\n",
        "# noise_dim is always 100\n",
        "# Input to generator is (B,noise_dim) where B is arbitray batch_size\n",
        "# Output of the Generator is (B,1,28,28) where B is arbitray batch_size, 1 is the grayscale channel 28 is image size \n",
        "# The Generator Output is between [-1,1] since we use tanh() activations. \n",
        "# Input to Discriminator is (B,1,28,28), where B is arbitray batch_size, 1 is the grayscale channel 28 is image size\n",
        "# output of the discriminator is Tensor of dimension (B,1) where B is arbitray batch_size \n",
        "\n",
        "a = torch.ones(5,100)\n",
        "if is_cuda:\n",
        "    a = a.cuda()\n",
        "out = generator(a)\n",
        "npt.assert_equal(out.shape, (5,1,28,28))\n",
        "assert np.max(out.detach().cpu().numpy()) <= 1\n",
        "assert np.min(out.detach().cpu().numpy()) >= -1\n",
        "\n",
        "# Hidden test cases follow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJuAqELYk_ku"
      },
      "source": [
        "### Discriminator Architecture - 20 points\n",
        "\n",
        "Define a Discriminator with the following architecture. \n",
        "\n",
        "- Linear Layer (input_size -> 512)\n",
        "- LeakyReLU with negative slope = 0.2\n",
        "- Linear Layer (512 -> 256)\n",
        "- LeakyReLU with negative slope = 0.2\n",
        "- Linear Layer (256 -> 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "deletable": false,
        "id": "ujIGE0n9k_ku",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "91b249b017e6f4f080d1525325be2357",
          "grade": false,
          "grade_id": "Discriminator_Define_soln",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "## Similar to the Generator, we now define a Discriminator which takes in a vector and output a single scalar \n",
        "## value. \n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size=784):\n",
        "        super(Discriminator, self).__init__()\n",
        "        '''\n",
        "        REST OF THE MODEL HERE\n",
        "        \n",
        "        # define a fully connected layer (self.layer1) from input_size -> 512 neurons   \n",
        "        # define a leaky relu layer(self.leaky_relu) with negative slope=0.2. (we will reuse the same layer)\n",
        "        # define a fully connected layer (self.layer2) from 512 -> 256 neurons\n",
        "        # define a fully connected layer (self.layer3) from 256 -> 1 neurons\n",
        "        '''\n",
        "        # your code here\n",
        "        self.layer1 = nn.Linear(input_size,512)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "        self.layer2 = nn.Linear(512,256)\n",
        "        self.layer3 = nn.Linear(256,1)\n",
        "\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        The Discriminator takes a vectorized input of the real and generated fake images. Reshape the input \n",
        "        to match the Discriminator architecture. \n",
        "        \n",
        "        Make a forward pass of the input through the Discriminator and return the scalar output of the \n",
        "        Discriminator.\n",
        "        '''\n",
        "        # your code here\n",
        "        #print(\"X Input shape and size - before\")\n",
        "        #print(x.shape)\n",
        "        #print(x.size())\n",
        "        N, C, H, W = x.size()\n",
        "        x = x.view(N,H*W)\n",
        "        #print(x.size())\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        #print(\"X Input shape and size - after\")\n",
        "        #print(x.shape)\n",
        "        #print(x.size())\n",
        "        y=x\n",
        "        #print(x.shape)\n",
        "        return y       \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "I5iKTwAtk_kv",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a8adedd7c55a85b56a368b8000b4ad32",
          "grade": false,
          "grade_id": "cell-f6fee4cf2037edf7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "e1052843-b3f0-4cbf-b1cb-b603c47aeaaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator(\n",
            "  (layer1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
            "  (layer2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (layer3): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Discriminator and move it to GPU (if is_cuda)\n",
        "discriminator = Discriminator()\n",
        "\n",
        "print(discriminator)\n",
        "# If you have a system with a GPU, you may want to install torchsummary and display the network in more detail \n",
        "# summary(discriminator,(784,), device='cpu')\n",
        "\n",
        "# move to GPU\n",
        "if is_cuda:\n",
        "    discriminator = discriminator.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "JsX6q5Bok_kv",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6d3b9715690d418cddae773fd6261687",
          "grade": true,
          "grade_id": "Discriminator_Define",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Test cases\n",
        "# Note the testcases only tests for input and output dimensions and range of values. \n",
        "# You may modify the architecture within those constraints\n",
        "# noise_dim is always 100\n",
        "# Input to generator is (B,noise_dim) where B is arbitray batch_size\n",
        "# Output of the Generator is (B,1,28,28) where B is arbitray batch_size, 1 is the grayscale channel 28 is image size \n",
        "# The Generator Output is between [-1,1] since we use tanh() activations. \n",
        "# Input to Discriminator is (B,1,28,28), where B is arbitray batch_size, 1 is the grayscale channel 28 is image size\n",
        "# output of the discriminator is Tensor of dimension (B,1) where B is arbitray batch_size \n",
        "\n",
        "a = torch.ones(5,1,28,28)\n",
        "if is_cuda:\n",
        "    a = a.cuda()\n",
        "out = discriminator(a)\n",
        "npt.assert_equal(out.shape, (5,1))\n",
        "\n",
        "# Hidden testcases follow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "aS-Jw0UJk_kv",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "75717cb1fb96bfa02d37204b7e408c1b",
          "grade": false,
          "grade_id": "cell-e4db11a2bd0e1f88",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Binary Cross Entropy Loss \n",
        "\n",
        "We will use the Binary cross entropy loss function to train the GAN. The loss function includes sigmoid activation followed by logistic loss. This allows us to distinguish between real and fake images.\n",
        "\n",
        "Binary cross entropy loss with logits: https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "RE9oG8fhk_kv",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4449babb4c263d1dc931864711d0367c",
          "grade": false,
          "grade_id": "cell-140aea5c6d03a0e8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Initialize the 'BCEWithLogitsLoss' object\n",
        "bce_loss = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0a-q5KNsN_6"
      },
      "source": [
        "### Discriminator Loss  - 10 points\n",
        "\n",
        "Let's define the objective function for the Discriminator. It takes as input the logits (outputs of the Discriminator) and the labels (real or fake). It uses the BCEWithLogitsLoss() to compute the loss in classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "deletable": false,
        "id": "tcWgqdIUk_kw",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e67d3f1f765d255c83236d8a281e81af",
          "grade": false,
          "grade_id": "DLoss_soln",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def DLoss(logits_real, logits_fake, targets_real, targets_fake):\n",
        "    '''\n",
        "    Returns the Binary Cross Entropy Loss between predictions and targets\n",
        "    \n",
        "    Inputs:\n",
        "        logits_real: the outputs of the discriminator (before the sigmoid) for real images\n",
        "        logits_fake: the outputs of the discriminator (before the sigmoid) for fake images\n",
        "        targets_real: groundtruth labels for real images\n",
        "        targets_fake: groundtruth labels for fake images\n",
        "    \n",
        "    '''\n",
        "    # Concatenate the logits_real and the logits_fake using torch.cat() to get 'logits'\n",
        "    # Concatenate the targets_real and the targets_fake using torch.cat() to get 'targets'\n",
        "    # estimate the loss using the BCEWithLogitsLoss object 'bce' with 'logits' and 'targets'\n",
        "    # your code here\n",
        "    logits = torch.cat(logits_real,logits_fake)\n",
        "    targets = torch.cat(targets_real, targets_fake)\n",
        "    loss = bce_loss(logits,targets)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "rlvp8ml_k_kw",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a1a5f397a63d6a7b779c7bf64c8db56a",
          "grade": true,
          "grade_id": "DLoss",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Hidden testcases follow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "413801e5a66386304d098d1263142508",
          "grade": false,
          "grade_id": "cell-8c62f1dceba005e0",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "hubqKv2SsN_6"
      },
      "source": [
        "### Generator Loss  - 10 points\n",
        "\n",
        "Let's define the objective function for the Generator. It takes as input the logits (outputs of the Discriminator) for the fake images it has generated and the labels (real). It uses the BCEWithLogitsLoss() to compute the loss in classification. \n",
        "The Generator expects the logits for the fake images it has generated to be close to 1 (real). If that is not the case, the Generatro corrects itself with the loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "deletable": false,
        "id": "j39Fr2Bwk_kw",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8f358ac2183ace252b7402a00a297f05",
          "grade": false,
          "grade_id": "GLoss_soln",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def GLoss(logits_fake, targets_real):\n",
        "    '''\n",
        "    The aim of the Generator is to fool the Discriminator into \"thinking\" the generated images are real.\n",
        "    GLoss is the binary cross entropy loss between the outputs of the Discriminator with the \n",
        "    generated fake images 'logits_fake' and real targets 'targets_real'\n",
        "    \n",
        "    Inputs: \n",
        "        logits_fake: Logits from the Discriminator for the fake images generated by the Generator\n",
        "        targets_real: groundtruth labels (close to 1) for the logits_fake\n",
        "    '''\n",
        "    # estimate the g_loss using the BCEWithLogitsLoss object 'bce' with 'logits_fake' and 'targets_real'\n",
        "    # your code here\n",
        "    g_loss = bce_loss(logits_fake,targets_real)\n",
        "    \n",
        "    return g_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "XTKfxZ_Ck_kx",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "dfefbf67d4aa78a1c9e2e0d099560656",
          "grade": true,
          "grade_id": "GLoss",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Hidden testcases follow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "xNwYVr-Sk_kx",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f4904f945896d448e088ce6482f8462c",
          "grade": false,
          "grade_id": "cell-b526b5602457942f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### GAN Training - 40 points\n",
        "\n",
        "Optimizers for training the Generator and the Discriminator. The below setup generates good images with the architecture. \n",
        "Feel free to adjust the optimizer settings. \n",
        "\n",
        "Adam optimizer: https://pytorch.org/docs/stable/optim.html#torch.optim.Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "IBCsLhoJk_kx"
      },
      "outputs": [],
      "source": [
        "#The following settings generated realistic images. Feel free to adjust the settings.\n",
        "epochs = 40\n",
        "noise_dim = 100\n",
        "LR = 0.0002\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=LR, betas=(0.5, 0.999))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LR, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "-vIbVC-xk_kx",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "904648b78f3a272a0013a85c741df58f",
          "grade": false,
          "grade_id": "cell-1c99013ac1384a3c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "## Training loop\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        \n",
        "        # We set targets_real and targets_fake to non-binary values(soft and noisy labels).\n",
        "        # This is a hack for stable training of GAN's.  \n",
        "        # GAN hacks: https://github.com/soumith/ganhacks#6-use-soft-and-noisy-labels\n",
        "        \n",
        "        targets_real = (torch.FloatTensor(images.size(0), 1).uniform_(0.8, 1.0))\n",
        "        targets_fake = (torch.FloatTensor(images.size(0), 1).uniform_(0.0, 0.2))\n",
        "                \n",
        "        if is_cuda:\n",
        "            targets_real = targets_real.cuda()\n",
        "            targets_fake = targets_fake.cuda()\n",
        "            images = images.cuda()\n",
        "        \n",
        "        ## D-STEP:\n",
        "        ## First, clear the gradients of the Discriminator optimizer.\n",
        "        ## Estimate logits_real by passing images through the Discriminator\n",
        "        ## Generate fake_images by passing random noise through the Generator. Also, .detach() the fake images \n",
        "        ## as we don't compute the gradients of the Generator when optimizing Discriminator.\n",
        "        ## fake_images = generator(noise(train_bs, noise_dim)).detach()\n",
        "        ## Estimate logits_fake by passing the fake images through the Discriminator\n",
        "        ## Compute the Discriminator loss by calling DLoss function.\n",
        "        ## Compute the gradients by backpropagating through the computational graph. \n",
        "        ## Update the Discriminator parameters.\n",
        "        \n",
        "        \n",
        "        ## G-STEP:\n",
        "        ## clear the gradients of the Generator. \n",
        "        ## Generate fake images by passing random noise through the Generator. \n",
        "        ## Estimate logits_fake by passing the fake images through the Discriminator.\n",
        "        ## compute the Generator loss by caling GLoss.\n",
        "        ## compute the gradients by backpropagating through the computational graph.\n",
        "        ## Update the Generator parameters. \n",
        "        \n",
        "        # your code here\n",
        "        # D-STEP\n",
        "        optimizer_D.zero_grad()\n",
        "        logits_real = discriminator(images)\n",
        "        fake_images = generator(noise(train_bs, noise_dim)).detach()\n",
        "        logits_fake = discriminator(fake_images)        \n",
        "        discriminator_loss = DLoss(logits_real,logits_fake,targets_real,targets_fake)\n",
        "        discriminator_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # G-STEP\n",
        "        optimizer_G.zero_grad()\n",
        "        fake_images = generator(noise(train_bs, noise_dim))\n",
        "        logits_fake = discriminator(fake_images)\n",
        "        generator_loss = GLoss(logits_fake, targets_real)\n",
        "        generator_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    print(\"Epoch:  \", epoch)\n",
        "    print(\"D Loss: \", discriminator_loss.item())\n",
        "    print(\"G Loss: \", generator_loss.item())\n",
        "          \n",
        "    if epoch % 2 == 0:\n",
        "        viz_batch = fake_images.data.cpu().numpy()\n",
        "        viz_batch = viz_batch[:100,:,:,:]\n",
        "        viz_batch = viz_batch.reshape(-1,28*28).squeeze()\n",
        "        viz_batch = viz_batch.reshape(10,10, 28,28).transpose(0,2,1,3).reshape(28*10,-1)\n",
        "\n",
        "        plt.figure(figsize = (8,8))\n",
        "        plt.axis('off')\n",
        "        plt.imshow(viz_batch, cmap='gray')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "25528ec3952bfcb890863513f57147b9",
          "grade": false,
          "grade_id": "GAN_images",
          "locked": true,
          "points": 40,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "-wmEujZLsN_7"
      },
      "source": [
        "#### The assignment is graded both manually and using auto-graded testcases."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}